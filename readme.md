# LinguaSense ğŸ™ï¸ğŸ§ 

**LinguaSense** is a multilingual, voice-activated Retrieval-Augmented Generation (RAG) assistant. It enables users â€” including those with visual impairments â€” to speak a natural language query into the system, retrieve relevant information from local documents, and hear an AI-generated response aloud in their native language.

---

## ğŸš€ Features

- ğŸ¤ **Live microphone input** with real-time speech capture
- ğŸ—£ï¸ **Multilingual speech-to-text** using Faster-Whisper (offline-capable)
- ğŸ“š **Semantic search** using Hugging Face embeddings + FAISS
- ğŸ¤– **Contextual response generation** with `google/gemma-7b-it` via Hugging Face Inference API
- ğŸ”Š **Multilingual TTS** using Coqui TTS with automatic language detection
- ğŸ§  Modular, local-first architecture â€” easy to extend and customize

---

## ğŸ“ Folder Structure

```
LinguaSense/
â”œâ”€â”€ app.py                  # Main app orchestration
â”œâ”€â”€ config.py               # API keys and model settings
â”œâ”€â”€ recorder.py             # Microphone-based audio recorder
â”œâ”€â”€ stt.py                  # Transcription via Faster-Whisper
â”œâ”€â”€ retriever.py            # Embeds + searches document chunks
â”œâ”€â”€ generator.py            # Uses Gemma LLM via HF API
â”œâ”€â”€ tts.py                  # Text-to-speech via Coqui
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ documents/              # Knowledge base (.txt files)
â”œâ”€â”€ audio_outputs/          # Output speech files
â””â”€â”€ models/                 # STT model files (Faster-Whisper)
```

---

## ğŸ”§ Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure `.env`

Create a `.env` file:

```env
HUGGINGFACE_API_KEY=your_hf_token
```

### 3. Download STT Model

Use `faster-whisper` to download a multilingual model (e.g., `small`, `base`, or `medium`).  
More info: https://github.com/guillaumekln/faster-whisper

Place it under `models/`.

---

## â–¶ï¸ Run the App

```bash
python app.py
```

Youâ€™ll be prompted to speak. The assistant will:
1. Record and transcribe your voice
2. Retrieve relevant text chunks from local documents
3. Use Gemma to generate a contextual answer
4. Speak the answer aloud

---

## ğŸ§ª Example Queries

- "Who is Sherlock Holmes?"
- "Summarize Pride and Prejudice in a sentence."
- "What is accessibility in web design?"

---

## ğŸ§  Powered By

- **Faster-Whisper** â€“ offline multilingual speech recognition
- **Hugging Face API** â€“ semantic embeddings + LLM (Gemma)
- **FAISS** â€“ fast similarity search over documents
- **Coqui TTS** â€“ multilingual speech synthesis
- **Langdetect** â€“ dynamic language detection

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Acknowledgements

Developed as part of the **Flickdone AI Platform Developer Challenge** to showcase applied multilingual AI for accessibility.
